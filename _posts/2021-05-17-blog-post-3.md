---
layout: post
title: HW 3 - Blog Post 3
---

In this blog post, we'll be exploring **Tensorflow** and creating a machine learning model that can classify fake news.

### Tensorflow

What is **Tensorflow**? Tensorflow is a machine learning package developed by Google. You can use it to develop your own machine learning models via tensors, which are essentially multidimensional arrays. By stacking layers, which take in a tensor and return another tensor of possibly a different shape, you obtain a complex model for your data.

### Preparing the Data and Creating the Dataset

Before we get to the fun part of creating our models, we must first prepare our fake news data and create a Tensorflow Dataset.

Let's import some of the packages we'll be using throughout this blog post.

- **Pandas**: used for reading in the data and performing some simple data analysis
- **Pyplot**: used for creating visualizations of the accuracy of our models
- **Tensorflow**: used for creating the machine learning models

```python
import pandas as pd
from matplotlib import pyplot as plt
import tensorflow as tf
```

We'll also read in the fake news data through which we'll train our models.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
fake_news = pd.read_csv(train_url)
fake_news = fake_news.drop("Unnamed: 0", axis = 1)
```

Let's examine the head of the data to see what we're working with.

```python
fake_news.head()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>


By looking at the head of the data, we can see that we have two potential **features** for our models: *title*, representing the headlines of our news, and *text*, representing the main body of our news. We will use these features to make predictions about our **label**, which is the *fake* column. Fake news is represented with a *0*, while real news is represented with a *1*.

Let's also take a look at how long our titles and texts are. This will come into play when we perform **text vectorization** later.

```python
max_text = ""
for text in fake_news["text"]:
  if len(text.split()) > len(max_text.split()):
    max_text = text
len(max_text.split())
```

    8117

```python
max_title = ""
for title in fake_news["title"]:
  if len(title.split()) > len(max_title.split()):
    max_title = title
len(max_title.split())
```

    41

Now that we've looked at the data, we can construct a **Tensorflow Dataset**. Let's define a *make_dataset* function that removes punctuation and stopwords from our titles and texts and also creates the Dataset. To do this, we'll use the **nltk** package, which is useful for natural language processing.

```python
import nltk
from nltk.corpus import stopwords
```

Note: Originally I used text standardization to remove capitals and punctuation after creating the Dataset. However, I found that many words similar to stopwords were still left over in the embedding layer. I think I have better results by first stripping the text of capitals and punctuation and then removing the stopwords. **nltk** provides a very simple way to remove capitals and punctuation using regex.

```python
def make_dataset(df):
  """
  This method creates a Tensorflow Dataset. First, the titles and texts are stripped of capitals, punctuation, and stopwords. Then, the Dataset is created with the title and text columns from the data as the inputs and the fake column as the output.

  Input:
  df: the Pandas dataframe containing the fake news data

  Output:
  data: the Tensorflow Dataset
  """
  stop = stopwords.words('english')
  tokenizer = nltk.RegexpTokenizer(r"\w+") # defining the tokenizer that splits the strings
  # removing the stopwords
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in tokenizer.tokenize(x) if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in tokenizer.tokenize(x) if word not in (stop)]))
  # creating the Dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        # inputs
        {
            "title" : df[["title"]],
            "text" : df[["text"]]
        },
        # output
        {
            "fake" : df[["fake"]]
        }
    )
  )
  data = data.batch(100) # batching the Dataset to train on chunks of data
  return data
```

Great! Now we can create a Tensorflow Dataset called data.

```python
data = make_dataset(fake_news)
```

Let's split our data into **training** data and **validation** data. We'll use the training data to train our models, and we'll hold back a tiny piece of our data to use as our validation data, which can be considered as data that the model hasn't yet seen.

```python
train_size = int(0.8 * len(data))
train = data.take(train_size) # taking 80% of our data to be the training data
```


```python
val_size = int(0.2 * len(data))
val = data.skip(train_size).take(val_size) # the remaining 20% is validation data
```

### Setting up the Models

Now we'll import all of the necessary packages from Tensorflow. We'll be using these to create our models.

```python
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
```

The first thing we need to do is create our **vectorization** layers. **Text vectorization** is a process in which we represent text as a vector. We replace each word in a title or text with its *frequency rank*, which represents the nth most common word in the Dataset. We create vectorization layers for both the titles and the texts.

```python
# only the top 2000 distinct words will be tracked
max_tokens_title = 2000

# each title will be a vector of length 50
sequence_length_title = 50

vectorize_layer_title = TextVectorization(
    max_tokens=max_tokens_title, # only consider this many words
    output_mode='int',
    output_sequence_length=sequence_length_title)

vectorize_layer_title.adapt(data.map(lambda x, y: x["title"]))
```

```python
# only the top 2000 distinct words will be tracked
max_tokens_text = 2000

# each text will be a vector of length 4000
sequence_length_text = 4000

vectorize_layer_text = TextVectorization(
    max_tokens=max_tokens_text, # only consider this many words
    output_mode='int',
    output_sequence_length=sequence_length_text)

vectorize_layer_text.adapt(data.map(lambda x, y: x["text"]))
```

Now, we'll define the inputs to our models. We specify *title_input* to represent the titles and *text_input* to represent the texts.

```python
# inputs

title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```

The last preparation we need to do is create a **shared embedding layer**. This layer will be shared by all three models we'll be creating.

```python
# embedding layer is shared by all three models
shared_embedding = layers.Embedding(max_tokens_text, 3, name = "embedding")
```

### Model 1: Title Only

Time to start creating some models! For this first model, we'll analyze how well we can predict fake news using only the news titles as an input. First, we stack our layers, starting with the vectorization layer and shared embedding layer and ending with a Dense layer that outputs 2 numbers (0 or 1).

```python
title_features = vectorize_layer_title(title_input)
title_features = shared_embedding(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(64, activation='sigmoid')(title_features)
```


```python
title = layers.Dense(64, activation='sigmoid')(title_features)
title_output = layers.Dense(2, name = "fake")(title)
```

Then, we create the model, specifiying that the input is *title_input* and the output is *title_output*.
```python
title_model = keras.Model(
    inputs = title_input,
    outputs = title_output
)
```

We can get an overview of our model by using the *summary()* method.

```python
title_model.summary()
```

Finally, we compile and fit the model with our training and validation data.

```python
title_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
title_history = title_model.fit(train,
            validation_data=val,
            epochs = 25,
            verbose = 1)
```

We've just completed our first model! Let's visualize the accuracy of our title-only model on the training and validation data. To do this, we will use *pyplot* to graph the change in accuracy over 25 epochs.

```python
plt.plot(title_history.history["accuracy"], label = "Training")
plt.plot(title_history.history["val_accuracy"], label = "Validation")
plt.title("Accuracy for Title Model")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
```

![title_model.png](/images/title_model.png)

We see that the accuracy of the validation data is actually higher than the accuracy of the training data in the earlier epochs. This might be due to the usage of a *dropout* layer, which reduces the number of units in each layer during training. However, later on the model begins to overfit, as the training accuracy is higher than the validation accuracy in later epochs.


We can also take a look at the max accuracy our model was able to achieve for both the training data and validation data.

```python
round(max(title_history.history["accuracy"]), 4)
```

    0.9904

```python
round(max(title_history.history["val_accuracy"]), 4)
```

    0.9854

It seems like our results were pretty good!

### Model 2: Text Only

We achieved high accuracy with our model that used only titles as an input. Let's try to build a model that uses only text as an input and compare the accuracy.

Again, we stack the layers to create the model. We will be using the same pipeline as the previous model.

```python
text_features = vectorize_layer_text(text_input)
text_features = shared_embedding(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(64, activation='sigmoid')(text_features)
```


```python
text = layers.Dense(64, activation='sigmoid')(text_features)
text_output = layers.Dense(2, name = "fake")(text)
```


```python
text_model = keras.Model(
    inputs = text_input,
    outputs = text_output
)
```

Now, we compile and fit the model.

```python
text_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
text_history = text_model.fit(train,
            validation_data=val,
            epochs = 25,
            verbose = 1)
```



```python
plt.plot(text_history.history["accuracy"], label = "Training")
plt.plot(text_history.history["val_accuracy"], label = "Validation")
plt.title("Accuracy for Text Model")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
plt.savefig("text.png")
```

![text_model.png](/images/text_model.png)

It seems like compared to the title-only model, the validation accuracy started out a little lower than the training accuracy and gradually increased. This time, we did not have as much overfitting and the validation accuracy remained higher than the training accuracy in the later epochs.

Let's see what were the max accuracies we achieved for the training data and validation data.

```python
round(max(text_history.history["accuracy"]), 4)
```

    0.9607

```python
round(max(text_history.history["val_accuracy"]), 4)
```

    0.9654

It seems like the accuracy for the text-only model is a little lower. With more training, I think we could probably achieve a better result, but for our purposes this is okay for now (we have better models).

### Model 3: Title and Text

Our final model will use both news titles and texts as inputs. We will use the same layers we used for the title-only model and text-only model.

```python
title_features = vectorize_layer_title(title_input)
title_features = shared_embedding(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(64, activation='sigmoid')(title_features)

text_features = vectorize_layer_text(text_input)
text_features = shared_embedding(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(64, activation='sigmoid')(text_features)
```

This time, since we are specifying two inputs, we need to concatenate our two pipelines. We do this using the *concatenate* function.

```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
main = layers.Dense(64, activation='sigmoid')(main)
output = layers.Dense(2, name = "fake")(main)
```


```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```

We'll compile the model and fit it to our training data and validation data.

```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
combined_history = model.fit(train,
            validation_data=val,
            epochs = 25,
            verbose = 1)
```

Let's see if the trends in the training accuracy and validation accuracy are any different from the previous models.

```python
plt.plot(combined_history.history["accuracy"], label = "Training")
plt.plot(combined_history.history["val_accuracy"], label = "Validation")
plt.title("Accuracy for Combined Title and Text Model")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
plt.savefig("combined.png")
```

![combined_model.png](/images/combined_model.png)

We see that the validation accuracy starts out significantly higher than the training accuracy. Validation accuracy also stays consistently higher than training accuracy.

Now, the moment of truth. Did we create a better model using both titles and texts rather than only one of them?

```python
round(max(combined_history.history["accuracy"]), 4)
```

    0.9814

```python
round(max(combined_history.history["val_accuracy"]), 4)
```

    0.9822

It seems like our combined model performed slightly worse than our title-only model. However, since the validation accuracy was quite good for both of them, I will test both of them using unseen data.

When detecting fake news, it seems like it's most effective to use either only the titles or both the titles and the text. Using only the text will probably be slightly less effective.

### Evaluating the Models Using Test Data

Let's see if our models have good accuracy on unseen data. First, we'll import the test data as a Pandas dataframe. This data belongs to the original fake news dataset but was split for the purposes of evaluating our models.

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test = pd.read_csv(test_url)
test = test.drop("Unnamed: 0", axis = 1)
```

We'll convert our test data into a Tensorflow Dataset.

```python
test = make_dataset(test)
```

Finally, we'll evaluate our models. The first entry represents the loss, and the second entry represents the accuracy. We'll first evaluate our title-only model, and then we'll evaluate our combined title and text model.

```python
title_model.evaluate(test)
```

    [0.37637969851493835, 0.9512673020362854]

```python
model.evaluate(test)
```

    [0.056399207562208176, 0.9811573028564453]

This is a little peculiar! Our title-only model managed to achieve a slightly higher validation accuracy when training for 25 epochs, but our combined title and text model managed to achieve a noticeably better accuracy on the test data. So, perhaps we should conclude that using both titles and text to predict fake news is the way to go!

### Embedding Visualization

For the last part of this blog post, we'll take a look at the embedding layer we used for all three of our models. A *word embedding* creates a way to look at words such that words with related meanings should be grouped closely together.

Let's import *plotly.express* to create visualizations of the embedding layer.

```python
import plotly.express as px
```

First, we'll look at the embedding using the vocabulary from the title vectorization layer.

```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer_title.get_vocabulary()                # get the vocabulary from the titles
```

Since visualizing in higher dimensions might be somewhat tricky, we'll use *PCA* to reduce the number of dimensions to 2.

```python
from sklearn.decomposition import PCA
```

```python
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab,
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```

```python
fig = px.scatter(embedding_df,
                 x = "x0",
                 y = "x1",
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```

{% include embedding_graph_title.html %}

Most of the words are clustered at the center. The words that are grouped together on the left have a lot to do with other countries, while words that are grouped together on the right seem to be mostly about the United States.

We'll also look at the embedding using the vocabulary from the text vectorization layer.

```python
weights = text_model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer_text.get_vocabulary()                # get the vocabulary from the text
```

```python
from sklearn.decomposition import PCA
```

```python
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab,
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```

```python
fig = px.scatter(embedding_df,
                 x = "x0",
                 y = "x1",
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```

{% include embedding_graph.html %}

Again, most of the words are clustered at the center. It seems that words that are grouped together on the left are very shocking words, while words that are grouped together on the right have a lot to do with government.
