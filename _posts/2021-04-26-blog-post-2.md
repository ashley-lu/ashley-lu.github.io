---
layout: post
title: HW 2 - Blog Post 2
---

In this blog post, we'll be exploring spectral clustering, a technique used to group data points into clusters. We will be making extensive use of numpy arrays and linear algebra to perform spectral clustering.

Let's import some of the necessary modules. We will be using numpy to perform linear algebra calculations, datasets from sklearn to make clusters from our data, and pyplot to graph our data and clusters.

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

### Introduction to Clustering

First, we look at an example in which we don't need spectral clustering. We use the `make_blobs` function to create two blobs of data. We set `X` to be the Euclidean coordinates of the points in the data, and we set `y` to be the labels of each point.

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![output_2_1.png](/images/output_2_1.png)

We can use K-means to cluster the data into two blobs:

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_4_1.png](/images/output_4_1.png)

### Harder Clustering

Does clustering still work, even on differently shaped data? Let's see an example in which the data is shaped into two crescents using the `make_moons` method.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![(output_6_1.png](/images/output_6_1.png)

Even though we can still identify two clusters of data, K-means doesn't work as well because it is used to identify circular clusters. In the below example, we attempt to use K-means and see that it doesn't accurately cluster the data:

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_8_1.png](/images/output_8_1.png)

Clearly, we did not obtain the result that we wanted. Throughout the rest of this code, we'll be implementing spectral clustering through several parts so that we can correctly cluster the data.

## Part A

First, we'll construct the *similarity matrix* **A** with shape `(n, n)` (where `n` is the number of data points).

We will also specify a parameter `epsilon` when constructing the similarity matrix. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). If not, then entry `A[i,j]` will be equal to `0`.

Also, **the diagonal entries `A[i,i]` should all be equal to zero.** We can use `np.fill_diagonal()` to set the diagonal entries to zero.

To calculate the *pairwise distances* (distances between each of the points in `X`), we will import metrics from the sklearn module to use its pairwise_distances method.

```python
from sklearn import metrics
```

We obtain a matrix of the pairwise distances in `X` by calling the pairwise_distances method and we call it `A`. Its shape is `(n, n)`, which in our case is the number of rows in `X`. Using the numpy method `where`, we can set distances values greater than `epsilon` to `0` and distance values less than `epsilon` to `1`. Finally, we use the fill_diagonal method so that all diagonal entries are equal to `0`.     

```python
epsilon = 0.4
A = metrics.pairwise_distances(X)
A = np.where(A > epsilon, 0, 1)
np.fill_diagonal(A, 0)
A
```

    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])

## Part B

Now that we have constructed the similarity matrix `A`, we can begin to cluster the data. Our goal in Part B is to compute the *binary norm cut objective*. A small binary norm cut objective indicates that the clusters are a good partition of the data.

The *binary norm cut objective* of a matrix A is the function

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

So, to compute the binary norm cut objective, we will first have to compute the cut term and the volume term.


#### B.1 The Cut Term

The *cut term* indicates how close the points in one cluster are to the points in another cluster.

We define a function called `cut(A,y)` to compute the cut term. The function iterates over the pairs of points in `A` and determines if the corresponding labels in `y` are in different clusters. If so, then the value at those coordinates in `A` will be added to the sum.

```python
def cut(A, y):
    """
    This function takes in a similarity matrix A and an array of labels y and computes the cut term, which indicates how close
    points in cluster 0 are to points in cluster 1. We use a nested for loop to iterate through each pair of points (i, j) in
    A. If for that point (i, j) the label of point i is different from the label of point j, then the points are in different
    clusters and we add that entry to the sum.
    """
    sum = 0
    for i in range(len(y)):
        for j in range(len(y)):
            if y[i] != y[j]:
                sum += A[i][j]

    return sum
```

{::options parse_block_html="true" /}
<div class="got-help">
Used len(y) instead of X.shape[0] as the range parameter since X is not an input parameter to the cut function.
</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="gave-help">
I think I made a good suggestion to one of my peers for the cut function. Instead of using list comprehension to find which points belong to which cluster, you can directly check in the nested for loop if the points belong to different clusters by indexing the list of labels y. If they belong to different clusters, then the sum is added to the cut.
</div>
{::options parse_block_html="false" /}

Let's test our cut function. We can compute the cut objective for the true clusters `y`, as well as compute the cut objective for a random vector of size `n` that contains labels that either equal 0 or 1.

```python
cut_term = cut(A, y)
cut_term
```

    26.0

```python
random = np.random.randint(2, size = n)
cut_term_random = cut(A, random)
cut_term_random
```

    2272.0

We find that the cut objective for the true clusters is much smaller than the cut objective for the random clusters.

#### B.2 The Volume Term

The second thing we need to compute for the binary norm cut objective is the *volume term*. The *volume* of a cluster measures how large a cluster is. Our goal for the binary norm cut objective is to find clusters that aren't too small, meaning that the volume terms for those clusters aren't small.

We define a function called `vols(A,y)` to compute the volume terms for two clusters **C0** and **C1**, and we return them as a tuple. Using numpy boolean indexing, we can compute the volume term for each cluster by summing up only rows that correspond to a specific label in `y`. For example, we can obtain the volume term for `C0` by only summing up rows in which `y` equals zero.

```python
def vols(A, y):
    """
    This function takes in a similarity matrix A and an array of labels y and computes the volume term, which indicates how
    large a cluster is. The volume term is the sum of all rows that correspond to the label of a cluster; for example, rows
    in A that correspond to the 0 label in y belong to cluster 0. We return the volumes of cluster 0 and cluster 1 as a tuple.
    """
    # obtain the sum of each row that belongs to c0, aka y[i] = 0
    c0 = A[y == 0]
    c0_sum = np.sum(c0)
    # obtain the sum of each row that belongs to c1, aka y[i] = 1
    c1 = A[y == 1]
    c1_sum = np.sum(c1)
    return (c0_sum, c1_sum)
```

We can test the vols function by passing in `A` and `y`.

```python
v0, v1 = vols(A, y)
v0
```

    2299.0

```python
v1
```

    2217.0

We see that both volume terms are large enough for our purposes of computing the binary norm cut objective.

Now that we have methods defining the cut term and the volume term, we can combine them to calculate the binary norm cut objective. We define a normcut function that calculates the binary norm cut objective according to the formula.

```python
def normcut(A, y):
    """
    This function takes in a similarity matrix A and an array of labels y and computes the binary norm cut objective. A small
    binary norm cut objective indicates that the clusters, cluster 0 and cluster 1, are a good partition of the data. We obtain
    the volumes of cluster 0 and cluster 1 using the vols method we defined above and return the value outputted by the norm cut
    function.
    """
    v0, v1 = vols(A, y)
    return cut(A, y) * (1/v0 + 1/v1)
```

Like what we did with the cut term, we can compare the `normcut` objective using both the true labels `y` and the random labels from before.

```python
print(normcut(A, y))
```

    0.02303682466323045

```python
print(normcut(A, random))
```

    2.0036976669014797

We observe that the `normcut` objective for the true labels is much smaller than the `normcut` objective for the random labels. This makes sense, since the cut term for the random labels was much larger than the cut term for the true labels.

## Part C

Finding a cluster vector `y` such that `normcut(A,y)` is small might be too hard to find in a relatively short amount of time, so we define a math trick. For a vector `z`, labels in `y` that equal zero correspond to `z` equaling `1 / vol(C0)`, and labels in `y` that equal one correspond to `z` equaling `-1 / vol(C1)`.

We define a function called transform that computes the `z` vector using the above formula. We use numpy boolean indexing to set the correct `z` values for indices in `z` that correspond to indices in `y`.

```python
def transform(A, y):
"""
This function takes in a similarity matrix A and an array of labels y and computes the z vector such that for indices of
y in which y is 0, then z is 1 / vol(c0), and for indices of y in which y is 1, then z is -1 / vol(c1).
"""
v0, v1 = vols(A, y)
z = 1 / v0 * (y == 0) -1 / v1 * (y == 1)
return z
```

{::options parse_block_html="true" /}
<div class="got-help">
When initializing the z vector, you can complete the boolean subsetting on the same line of code as the initialization by multiplying the z values with the boolean subsets. This is a neat trick that completes the task in one line of code rather than my original three lines of code.
</div>
{::options parse_block_html="false" /}

Using linear algebra, we can check to see if the below equation holds:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where `D` is the diagonal matrix such that its diagonal entries equal the sums of the corresponding rows.

We compute the right side of the linear algebra equation, using `z` obtained from the above function and a diagonal matrix `D` obtained from the numpy diagonal method. Since computer arithmetic can be slightly off, we use the numpy method `isclose` to check if the norm cut is *close enough* to the right side of the equation.

```python
z = transform(A, y)
D = np.diag(A.sum(axis = 1))
eq = 2 * (z.T @ (D - A) @ z) / (z.T @ D @z)
np.isclose(normcut(A, y), eq)
```

    True

We get that the two sides of the equation are equal.

We can also check to see if $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). If the identity holds, that means `z` contains roughly about as many positive entries as negative entries.

```python
one = np.ones(n)
np.isclose(z.T @ D @ one, 0)
```

    True

Again, we get that our identity holds.

## Part D

From the previous part, we computed a linear algebra equation that is equal to the norm cut in our goal to minimize the norm cut. We can use `scipy.optimize` to minimize our `z` while also including the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ in the optimization. We do this by defining an `orth_obj` function and calling it on a random point `n`.

```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n)

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

```python
# minimizing orth_obj with respect to z
import scipy.optimize
z_ = scipy.optimize.minimize(orth_obj, np.random.rand(n))
```

{::options parse_block_html="true" /}
<div class="got-help">
When minimizing the norm cut, instead of calling `minimize` on `z`, instead we call `minimize` on a random point `n`, which produces a much better clustering result compared to what I had previously!
</div>
{::options parse_block_html="false" /}

We get an array that we define as `z_`. To find `z_min`, which is the solution of the optimization, we set it equal to `z_.x`.

```python
z_min = z_.x
```

## Part E

Now we can try to cluster the data using `z_min`. Data points in `X` where `z_min[i] >= 0` belong to one cluster, while data points in X where `z_min[i] < 0` belong to another cluster.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
color = np.where(z_min >= 0, 0, 1)
plt.scatter(X[:,0], X[:,1], c = color)
```

![output_55.png](/images/output_55.png)

We see that we obtain a great result! The two crescents are clustered almost perfectly.

## Part F

We can implement the process that we defined in **Part D** and **Part E** using *eigenvalues* and *eigenvectors* instead, which have been widely studied and used.

We can construct the *Laplacian* matrix and then use it to derive the eigenvalues and eigenvectors. Then, we sort the eigenvalues and eigenvectors from smallest to largest and select the second smallest eigenvector to be our `z_eig` value.

```python
# constructing the Laplacian matrix
L = np.linalg.inv(D) @ (D - A)
# computing the eigenvalues and eigenvectors
Lam, U = np.linalg.eig(L)

ix = Lam.argsort()

# sorting the eigenvalues and eigenvectors from smallest to largest
Lam, U = Lam[ix], U[:,ix]

# 2nd smallest eigenvalue and corresponding eigenvector
Lam[1], U[:,1]
z_eig = np.where(U[:,1] >= 0, 0, 1)
```

Now, we use `z_eig` to cluster the data. Data points in `X` where `z_eig < 0` belong to one cluster, while data points in X where `z_eig >= 0` belong to another cluster.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = z_eig)
```

![corrected2.png](/images/corrected2.png)

We see that we come pretty close to clustering the data correctly!

## Part G

Now, we will put together everything that we've accomplished through the previous parts and write a `spectral_clustering(X, epsilon)` function that will perform spectral clustering. We pass in our array of data points `X` and a distance parameter `epsilon` and return an array of binary labels indicating whether data point `i` is in group `0` or group `1`.

First, we construct the similarity matrix and the diagonal matrix necessary to compute the Laplacian matrix. Then, we obtain the eigenvalues and eigenvectors from the Laplacian matrix and sort them from smallest to largest. We use the sign of the second smallest eigenvector to determine which group each point is in.

```python
def spectral_clustering(X, epsilon):
  """
  This function takes in input data X and a distance threshold epsilon and performs spectral clustering, returning an array
  of binary labels indicating whether each data point is in cluster 0 or cluster 1. First, we construct the similarity
  matrix A using X and epsilon, and the diagonal matrix from part C using A. Then, we construct the Laplacian matrix using
  A and D, obtaining the eigenvalues and eigenvectors. We obtain the second smallest eigenvector z_eig and use it to construct
  the array of labels. z_eig values greater than or equal to 0 belong to one cluster, while z_eig values less than 0 belong
  to another cluster.

  Input: the array of data points X, the distance parameter epsilon
  Output: the list of cluster labels z_eig
  """
  A = metrics.pairwise_distances(X) # computes the distances between points in X
  A = np.where(A > 0.4, 0, 1) # fills the A matrix with 0s and 1s depending on the magnitude of the pairwise distance
  np.fill_diagonal(A, 0) # fills the diagonal of A with 0s
  D = np.diag(A.sum(axis = 1)) # computes the diagonal matrix defined in part C
  L = np.linalg.inv(D) @ (D - A) # computes the Laplacian matrix
  Lam, U = np.linalg.eig(L) # obtains the eigenvalues and eigenvectors
  ix = Lam.argsort() # defines the sorting parameter
  Lam, U = Lam[ix], U[:,ix] # sorts the eigenvalues and eigenvectors from smallest to largest
  z_eig = np.where(U[:,1] >= 0, 0, 1) # constructing the array of labels using the second smallest eigenvector
  return z_eig
```

{::options parse_block_html="true" /}
<div class="got-help">
Added additional documentation to the spectral_clustering method for clarity.
</div>
{::options parse_block_html="false" /}

Great! Now that we have a compact `spectral_clustering` function, we can pass it as a color argument to the `scatter` function and see how we did with the clustering. The result should be the same as what we obtained in **Part F**.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![png](/images/output_65_1.png)

We see that we obtain the same clusters as **Part F**.

## Part H

Now, we can use our `spectral_clustering` function to run some experiments. For example, what happens when we increase the `noise` to `0.2` and the number of samples `n` to `1000`?

```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![output_67_1.png](/images/output_67_1.png)

Uh-oh. Seems like the clustering doesn't work quite so well on larger `noise` parameters. Instead of clustering in crescent shapes, we see that the clusters are more circular/blob-like.

What if we lower the `noise` to `0.01`?

```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.01, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![output_68_1.png](/images/output_68_1.png)

We see that spectral clustering works extremely well on low `noise` parameters. It seems like `noise` affects the variance in the data. A higher `noise` parameter will cause the data points to be much more spread out, creating issues for spectral clustering. On the other hand, a lower `noise` parameter will cause the data points to be closer together, making it easier for spectral clustering to determine the clusters.

{::options parse_block_html="true" /}
<div class="got-help">
Added some additional noise testing to give the reader a better understanding of how noise affects spectral clustering.
</div>
{::options parse_block_html="false" /}

## Part I

We can also test our `spectral_clustering` function on different data sets, such as the bull's eye data set, which is represented by concentric circles.

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![output_69_1.png](/images/output_69_1.png)

Similarly to the crescent shaped data, K-means doesn't work so well here either.

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_71_1.png](/images/output_71_1.png)

Let's see if we can use our `spectral_clustering` function to correctly cluster the two concentric circles. We test various `epsilon` values and observe which one comes the closest to clustering correctly.

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![output_73_1.png](/images/output_73_1.png)

For this test, we specify our `epsilon` value to be `0.4`, which is the same parameter we used for the crescent shaped data. We see that the clustering is pretty accurate. All of the points on the outer circle belong to one cluster, while all of the points on the inner circle belong to another cluster.

How about if we lower the `epsilon` value?

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```

![output_74_1.png](/images/output_74_1.png)

For this test, we specify our `epsilon` value to be `0.2`. We see that we run into some problems; it seems that our `spectral_clustering` function has clustered all of the points in one group.

Finally, we try raising the `epsilon` value.

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.9))
```

![output_75_1.png](/images/output_75_1.png)

For this test, we specify our `epsilon` value to be `0.9`. Again, we run into some problems; this time, it seems that our `spectral_clustering` function has drawn some sort of invisible line through the data. Everything above that line belongs to one cluster, while everything below that line belongs to another cluster.

So, it seems that for the bull's eye data set, the optimal `epsilon` value hovers around `0.4`.

{::options parse_block_html="true" /}
<div class="gave-help">
For the most part I think my peers' code and writing were very good, so I didn't need to point out many tricks to make their code more efficient. Instead, I mostly suggested things to make their code more readable; for example, using .T instead of .transpose() in a very long line of code that defines the linear algebra equation makes the code a little less messy. Also, I suggested to some of my peers to use more of their own explanations when it came to talking about the functions they wrote. To a PIC 16A student, I think reading some of the code for the first time could be a bit confusing. When I was first doing the assignment I was confused on how to write the cut and volume functions, so I suggested that some of my peers explain their coding process so it's easier for the reader to understand.
</div>
{::options parse_block_html="false" /}
